{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import keras\n",
    "sys.path.append('../../../LIB/')\n",
    "from utils import *\n",
    "from customClasses.utils import *\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', 100, 'display.width', 1024)\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['axes.facecolor'] = '#12162f'\n",
    "plt.grid(False)\n",
    "plt.close()\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "SPLIT_DATE = '2019-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[data overview](https://doc-0s-bg-docs.googleusercontent.com/docs/securesc/ehn0gf3bao47bmq1qaa15nqo4thqnecu/kb0opjk2seh3r2eb02a77m7j1fqklmff/1572861600000/06603566150954428909/05576845911769771244/1QdxvAXep1_kJX_CzpGnwkGz3qE6_pgkc?h=08654040859286330787)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV casitng dates\n",
    "DATA_PATH = './data'\n",
    "answers = pd.read_csv(os.path.join(DATA_PATH, 'answers.csv'))\n",
    "answer_scores = pd.read_csv(os.path.join(DATA_PATH, 'answer_scores.csv'))\n",
    "comments = pd.read_csv(os.path.join(DATA_PATH, 'comments.csv'))\n",
    "emails = pd.read_csv(os.path.join(DATA_PATH, 'emails.csv'))\n",
    "groups = pd.read_csv(os.path.join(DATA_PATH, 'groups.csv'))\n",
    "group_memberships = pd.read_csv(os.path.join(DATA_PATH, 'group_memberships.csv'))\n",
    "matches = pd.read_csv(os.path.join(DATA_PATH, 'matches.csv'))\n",
    "professionals = pd.read_csv(os.path.join(DATA_PATH, 'professionals.csv'))\n",
    "questions = pd.read_csv(os.path.join(DATA_PATH, 'questions.csv'))\n",
    "question_scores = pd.read_csv(os.path.join(DATA_PATH, 'question_scores.csv')) \n",
    "school_memberships = pd.read_csv(os.path.join(DATA_PATH, 'school_memberships.csv'))\n",
    "students = pd.read_csv(os.path.join(DATA_PATH, 'students.csv'))\n",
    "tags = pd.read_csv(os.path.join(DATA_PATH, 'tags.csv'))\n",
    "tag_questions = pd.read_csv(os.path.join(DATA_PATH, 'tag_questions.csv'))\n",
    "tag_users = pd.read_csv(os.path.join(DATA_PATH, 'tag_users.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetimes features preprocessing\n",
    "answers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'], infer_datetime_format=True)\n",
    "comments['comments_date_added'] = pd.to_datetime(comments['comments_date_added'], infer_datetime_format=True)\n",
    "emails['emails_date_sent'] = pd.to_datetime(emails['emails_date_sent'], infer_datetime_format=True)\n",
    "professionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'], infer_datetime_format=True)\n",
    "questions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'], infer_datetime_format=True)\n",
    "students['students_date_joined'] = pd.to_datetime(students['students_date_joined'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Answer\n",
    "temp = answers.groupby('answers_author_id')['answers_date_added'].max()\n",
    "professionals['date_last_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_answer')), left_on='professionals_id', right_index=True, how='left')['last_answer']\n",
    "# First Answer\n",
    "temp = answers.groupby('answers_author_id')['answers_date_added'].min()\n",
    "professionals['date_first_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_answer')), left_on='professionals_id', right_index=True, how='left')['first_answer']\n",
    "# Last Comment\n",
    "temp = comments.groupby('comments_author_id')['comments_date_added'].max()\n",
    "professionals['date_last_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_comment')), left_on='professionals_id', right_index=True, how='left')['last_comment']\n",
    "# First Comment\n",
    "temp = comments.groupby('comments_author_id')['comments_date_added'].min()\n",
    "professionals['date_first_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_comment')), left_on='professionals_id', right_index=True, how='left')['first_comment']\n",
    "# Last Activity\n",
    "professionals['date_last_activity'] = professionals[['date_last_answer', 'date_last_comment']].max(axis=1)\n",
    "# First Activity\n",
    "professionals['date_first_activity'] = professionals[['date_first_answer', 'date_first_comment']].min(axis=1)\n",
    "\n",
    "# Last activity (Question)\n",
    "temp = questions.groupby('questions_author_id')['questions_date_added'].max()\n",
    "students['date_last_question'] = pd.merge(students, pd.DataFrame(temp.rename('last_question')), left_on='students_id', right_index=True, how='left')['last_question']\n",
    "# First activity (Question)\n",
    "temp = questions.groupby('questions_author_id')['questions_date_added'].min()\n",
    "students['date_first_question'] = pd.merge(students, pd.DataFrame(temp.rename('first_question')), left_on='students_id', right_index=True, how='left')['first_question']\n",
    "# Last activity (Comment)\n",
    "temp = comments.groupby('comments_author_id')['comments_date_added'].max()\n",
    "students['date_last_comment'] = pd.merge(students, pd.DataFrame(temp.rename('last_comment')), left_on='students_id', right_index=True, how='left')['last_comment']\n",
    "# First activity (Comment)\n",
    "temp = comments.groupby('comments_author_id')['comments_date_added'].min()\n",
    "students['date_first_comment'] = pd.merge(students, pd.DataFrame(temp.rename('first_comment')), left_on='students_id', right_index=True, how='left')['first_comment']\n",
    "# Last activity (Total)\n",
    "students['date_last_activity'] = students[['date_last_question', 'date_last_comment']].max(axis=1)\n",
    "# First activity (Total)\n",
    "students['date_first_activity'] = students[['date_first_question', 'date_first_comment']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextProcessor()\n",
    "# processing text for answers entity\n",
    "answers['answers_body'] = answers['answers_body'].astype(str)\n",
    "answers = tp.transform(answers,column='answers_body')\n",
    "ans_train = answers[answers['answers_date_added'] < SPLIT_DATE]\n",
    "\n",
    "# processing text for questions entity\n",
    "questions = tp.transform(questions,column='questions_title')\n",
    "questions = tp.transform(questions,column='questions_body')\n",
    "questions['questions_whole'] = questions['questions_title'] + ' ' + questions['questions_body']\n",
    "que_train = questions[questions['questions_date_added'] < SPLIT_DATE]\n",
    "\n",
    "\n",
    "# processing text for professionals entity\n",
    "professionals['professionals_headline'] = professionals['professionals_headline'].astype(str)\n",
    "professionals['professionals_industry'] = professionals['professionals_industry'].astype(str)\n",
    "\n",
    "professionals = tp.transform(professionals,column='professionals_headline')\n",
    "professionals = tp.transform(professionals,column='professionals_industry')\n",
    "pro_train = professionals[professionals['professionals_date_joined'] < SPLIT_DATE]\n",
    "\n",
    "stu_train = students[students['students_date_joined'] < SPLIT_DATE]\n",
    "que_train = questions[questions['questions_date_added'] < SPLIT_DATE]\n",
    "\n",
    "\n",
    "tags['tags_tag_name'] = tags['tags_tag_name'].astype(str)\n",
    "tags['tags_tag_name'] = tags['tags_tag_name'].apply(lambda x: tp.process(x, allow_stopwords=True))\n",
    "\n",
    "tag_que = tag_questions.merge(tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\n",
    "tag_pro = tag_users.merge(tags, left_on='tag_users_tag_id', right_on='tags_tag_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def train_d2v(df: pd.DataFrame, target: str, features: list, dim: int) -> (Doc2Vec, dict):\n",
    "    \"\"\"\n",
    "    Train Doc2Vec object on provided data\n",
    "    :param df: data to work with\n",
    "    :param target: column name of target entity in df to train embeddings for\n",
    "    :param features: list of feature names to be used for training\n",
    "    :param dim: dimension of embedding vectors to train\n",
    "    :return: trained Doc2Vec object\n",
    "    \"\"\"\n",
    "    prepared = []\n",
    "    for feature in features:\n",
    "        if feature != target:\n",
    "            prepared += [TaggedDocument(row[feature].split(), [row[target]])\n",
    "                         for i, row in df[[feature, target]].drop_duplicates().iterrows()]\n",
    "        else:\n",
    "            prepared += [TaggedDocument(s.split(), [s]) for s in df[target].drop_duplicates()]\n",
    "    # shuffle prepared data, just in case\n",
    "    prepared = random.sample(prepared, len(prepared))\n",
    "    d2v = Doc2Vec(prepared, vector_size=dim, workers=4, epochs=10, dm=0)\n",
    "    docvecs = {d2v.docvecs.index2entity[i]: d2v.docvecs.vectors_docs[i]\n",
    "               for i in range(len(d2v.docvecs.index2entity))}\n",
    "    return d2v, docvecs\n",
    "\n",
    "\n",
    "def pipeline_d2v(que: pd.DataFrame, ans: pd.DataFrame, pro: pd.DataFrame, tag_que: pd.DataFrame, tag_pro: pd.DataFrame,\n",
    "                 dim: int) -> (dict, dict, Doc2Vec):\n",
    "    \"\"\"\n",
    "    Pipeline for training embeddings for\n",
    "    professional's industries and question's tags via doc2vec algorithm\n",
    "    on question titles, bodies, answer bodies, names of tags, professional industries and headlines\n",
    "\n",
    "    :param que: raw questions.csv dataset\n",
    "    :param ans: raw answers.csv dataset\n",
    "    :param pro: raw professionals.csv dataset\n",
    "    :param tag_que: tags.csv merged with tag_questions.csv\n",
    "    :param tag_pro: tags.csv merged with tag_users.csv\n",
    "    :param dim: dimension of doc2vec embeddings to train\n",
    "    :return: trained tags, industries embeddings and question's Doc2Vec model\n",
    "    \"\"\"\n",
    "    # aggregate all the tags in one string for same professionals\n",
    "    pro_tags = tag_pro[['tag_users_user_id', 'tags_tag_name']].groupby(by='tag_users_user_id', as_index=False) \\\n",
    "        .aggregate(lambda x: ' '.join(x)).rename(columns={'tags_tag_name': 'tags_pro_name'})\n",
    "    pro_tags = pro.merge(pro_tags, left_on='professionals_id', right_on='tag_users_user_id')\n",
    "\n",
    "    # merge questions, tags, answers and professionals\n",
    "    que_tags = que.merge(tag_que, left_on='questions_id', right_on='tag_questions_question_id')\n",
    "    ans_que_tags = ans.merge(que_tags, left_on=\"answers_question_id\", right_on=\"questions_id\")\n",
    "    df = ans_que_tags.merge(pro_tags, left_on='answers_author_id', right_on='professionals_id')\n",
    "\n",
    "    text_features = ['questions_title', 'questions_body', 'answers_body', 'tags_tag_name', 'tags_pro_name',\n",
    "                     'professionals_industry', 'professionals_headline']\n",
    "\n",
    "    # train and save question's tags embeddings\n",
    "    _, tags_embs = train_d2v(df, 'tags_tag_name', text_features, dim)\n",
    "\n",
    "    # aggregate all the tags in one string for same questions\n",
    "    que_tags = que_tags[['questions_id', 'tags_tag_name']].groupby(by='questions_id', as_index=False) \\\n",
    "        .aggregate(lambda x: ' '.join(x))\n",
    "\n",
    "    # merge questions, aggregated tags, answers and professionals\n",
    "    que_tags = que.merge(que_tags, on='questions_id')\n",
    "    ans_que_tags = ans.merge(que_tags, left_on=\"answers_question_id\", right_on=\"questions_id\")\n",
    "    df = ans_que_tags.merge(pro_tags, left_on='answers_author_id', right_on='professionals_id')\n",
    "\n",
    "    # train and save professional's industries embeddings\n",
    "    _, inds_embs = train_d2v(df, 'professionals_industry', text_features, dim)\n",
    "\n",
    "    head_d2v, _ = train_d2v(df, 'professionals_headline', text_features, 5)\n",
    "\n",
    "    ques_d2v, _ = train_d2v(que_tags, 'questions_id', ['questions_whole'], dim)\n",
    "\n",
    "    return tags_embs, inds_embs, head_d2v, ques_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 10\n",
    "#trained tags, industries embeddings and question's Doc2Vec model\n",
    "tag_embs, ind_embs, head_d2v, ques_d2v = pipeline_d2v(que_train, ans_train, pro_train, tag_que, tag_pro, n_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def vis_emb(id_to_vec: dict, occurrences: pd.Series, title: str):\n",
    "    \"\"\"\n",
    "    Visualize embeddings via T-SNE\n",
    "    \"\"\"\n",
    "    top = set(occurrences.value_counts().iloc[:100].index)\n",
    "    filtered = {key: value for key, value in id_to_vec.items() if key in top}\n",
    "    \n",
    "    proj = TSNE(n_components=2).fit_transform(np.vstack(filtered.values()))\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(12, 12))\n",
    "    plt.scatter(proj[:, 0], proj[:, 1], alpha=0.7, s=90, c='#69f0ff')\n",
    "    for i, name in enumerate(filtered.keys()):\n",
    "        ax.annotate(name, (proj[i, 0], proj[i, 1]), color='w')\n",
    "        \n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_emb(tag_embs, tag_que['tags_tag_name'], 'Doc2Vec tags embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_emb(ind_embs, pro_train['professionals_industry'], 'Doc2Vec industries embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataframe used to extract positive pairs\n",
    "pairs_df = questions.merge(answers, left_on='questions_id', right_on='answers_question_id') \\\n",
    "    .merge(professionals, left_on='answers_author_id', right_on='professionals_id') \\\n",
    "    .merge(students, left_on='questions_author_id', right_on='students_id')\n",
    "\n",
    "pairs_df = pairs_df[['questions_id', 'students_id', 'professionals_id', 'answers_date_added']]\n",
    "\n",
    "# extract positive pairs\n",
    "pos_pairs = list(pairs_df.loc[pairs_df['answers_date_added'] < SPLIT_DATE].itertuples(index=False, name=None))\n",
    "\n",
    "# extract professional answers for activity filters\n",
    "pro_answers = pairs_df.drop(columns=['students_id']).set_index('professionals_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = questions.merge(answers, left_on='questions_id', right_on='answers_question_id')\n",
    "qa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#### Question's features processing class\n",
    "- **Numerical**\n",
    "    - Question's body length in symbols\n",
    "    - Question's number of tags\n",
    "- Averaged question's tags embeddings pre-trained via doc2vec\n",
    "- Unique question's embedding inferred via doc2vec \n",
    "\"\"\"\n",
    "\n",
    "class process_quest_features:\n",
    "    \"\"\"\n",
    "    :que: question dataFrame with preprocessed dataset\n",
    "    :tags: merged tags and tag_questions dataframes with preprocessed textual columns\n",
    "    :return: dataframe of question's id, question's date added and model-friendly question's features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, que, tags):\n",
    "        que['questions_body_len'] = ques['questions_body'].apply(lambda x:len(str(x)))\n",
    "        # append aggregated tags to each question\n",
    "        tags_grouped = tags.groupby('tag_questions_question_id', as_index=False)[['tags_tag_name']] \\\n",
    "            .agg(lambda x: ' '.join(set(x)))\n",
    "        tags_grouped['questions_tag_count'] = tags_grouped['tags_tag_name'].apply(lambda x: len(x.split()))\n",
    "        df = que.merge(tags_grouped, how='left', left_on='questions_id', right_on='tag_questions_question_id')\n",
    "        \n",
    "        # prepare tag embeddings\n",
    "\n",
    "        tag_emb_len = list(self.tag_embs.values())[0].shape[0]\n",
    "\n",
    "        def __convert(s):\n",
    "            embs = []\n",
    "            for tag in str(s).split():\n",
    "                if tag in self.tag_embs:\n",
    "                    embs.append(self.tag_embs[tag])\n",
    "            if len(embs) == 0:\n",
    "                embs.append(np.zeros(tag_emb_len))\n",
    "            return np.vstack(embs).mean(axis=0)\n",
    "\n",
    "        mean_embs = df['tags_tag_name'].apply(__convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data['duration_to_answer'] =  pd.to_datetime(qa_data['answers_date_added']) - pd.to_datetime(qa_data['questions_date_added'])\n",
    "# qa_data['duration_to_answer'].dt.seconds.plot(kind='hist', figsize=(15,10), bins=200, density=True)\n",
    "que_age = qa_data['duration_to_answer'].apply(lambda x: x.days + np.round(x.seconds / 3600) / 24)\n",
    "cut_que_age = que_age[(que_age < 100) & (que_age > 3)]\n",
    "cut_que_age.hist(bins=100, density=True)\n",
    "\n",
    "lambd = 1 / 20\n",
    "\n",
    "# Plot exponential distribution\n",
    "x = np.linspace(0, 100, 1000)\n",
    "y = lambd * np.exp(-lambd * x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Class to ingest data from pre-processed DataFrames to model\n",
    "    in form of batches of NumPy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_mean = 30\n",
    "    \n",
    "    def __init__(self, que: pd.DataFrame, stu: pd.DataFrame, pro: pd.DataFrame,\n",
    "                 batch_size: int, pos_pairs: list, nonneg_pairs: list, pro_dates: dict):\n",
    "        \"\"\"\n",
    "        :param que: pre-processed questions data\n",
    "        :param stu: pre-processed students data\n",
    "        :param pro: pre-processed professionals data\n",
    "        :param batch_size: actually, half of the real batch size\n",
    "        Number of both positive and negative pairs present in generated batch\n",
    "        :param pos_pairs: tuples of question, student and professional, which form positive pair\n",
    "        (professional answered on the given question from corresponding student)\n",
    "        :param nonneg_pairs: tuples of question, student and professional, which are known to form a positive pair.\n",
    "        Superset of pos_pairs, used in sampling of negative pairs\n",
    "        :param pro_dates: mappings from professional's id to his registration date\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # extract mappings from question's id to question's date and features\n",
    "        que_ar = que.values\n",
    "        self.que_feat = {que_ar[i, 0]: que_ar[i, 2:] for i in range(que_ar.shape[0])}\n",
    "        self.que_time = {que_ar[i, 0]: pd.Timestamp(que_ar[i, 1]) for i in range(que_ar.shape[0])}\n",
    "\n",
    "        self.pos_pairs = pos_pairs\n",
    "        self.on_epoch_end()  # shuffle pos_pairs\n",
    "        self.nonneg_pairs = {(que, stu, pro) for que, stu, pro, time in nonneg_pairs}\n",
    "\n",
    "        # these lists are used in sampling of negative pairs\n",
    "        self.ques_stus_times = [(que, stu, self.que_time[que]) for que, stu, pro, time in pos_pairs]\n",
    "\n",
    "        self.pros = np.array([pro for que, stu, pro, time in nonneg_pairs])\n",
    "        self.pros_times = np.array([pro_dates[pro] for que, stu, pro, time in nonneg_pairs])\n",
    "\n",
    "        # simultaneously sort two arrays containing professional features\n",
    "        sorted_args = np.argsort(self.pros_times)\n",
    "        self.pros = self.pros[sorted_args]\n",
    "        self.pros_times = self.pros_times[sorted_args]\n",
    "\n",
    "        # extract mappings from student's id to student's date and features\n",
    "        self.stu_feat = {}\n",
    "        self.stu_time = {}\n",
    "        for stu_id, group in stu.groupby('students_id'):\n",
    "            group_ar = group.values[:, 1:]\n",
    "            self.stu_feat[stu_id] = np.array([group_ar[i, 1:] for i in range(group_ar.shape[0])])\n",
    "            self.stu_time[stu_id] = np.array([group_ar[i, 0] for i in range(group_ar.shape[0])])\n",
    "\n",
    "        # extract mappings from professional's id to professional's date and features\n",
    "        self.pro_feat = {}\n",
    "        self.pro_time = {}\n",
    "        for pro_id, group in pro.groupby('professionals_id'):\n",
    "            group_ar = group.values[:, 1:]\n",
    "            self.pro_feat[pro_id] = np.array([group_ar[i, 1:] for i in range(group_ar.shape[0])])\n",
    "            self.pro_time[pro_id] = np.array([group_ar[i, 0] for i in range(group_ar.shape[0])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos_pairs) // self.batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def __find(feat_ar: np.ndarray, time_ar: np.ndarray, search_time):\n",
    "        pos = np.searchsorted(time_ar[1:], search_time)\n",
    "        assert time_ar[pos] is pd.NaT or time_ar[pos] < search_time\n",
    "        return feat_ar[pos]\n",
    "\n",
    "    def __convert(self, pairs: list) -> (np.ndarray, np.ndarray):\n",
    "        \"\"\"\n",
    "        Convert list of pairs of ids to NumPy arrays\n",
    "        of question and professionals features\n",
    "        \"\"\"\n",
    "        x_que, x_pro, current_times = [], [], []\n",
    "        for que, stu, pro, current_time in pairs:\n",
    "            que_data = self.que_feat[que]\n",
    "\n",
    "            # find student's and professional's feature at current time\n",
    "            stu_data = BatchGenerator.__find(self.stu_feat[stu], self.stu_time[stu], current_time)\n",
    "            pro_data = BatchGenerator.__find(self.pro_feat[pro], self.pro_time[pro], current_time)\n",
    "\n",
    "            # prepare current time as feature itself\n",
    "            current_time = current_time.year + (current_time.dayofyear + current_time.hour / 24) / 365\n",
    "            current_times.append(current_time)\n",
    "\n",
    "            x_que.append(np.hstack([stu_data, que_data]))\n",
    "            x_pro.append(pro_data)\n",
    "\n",
    "        # and append them to both questions and professionals\n",
    "        return np.vstack(x_que), np.vstack(x_pro)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate the batch\n",
    "        \"\"\"\n",
    "        pos_pairs = self.pos_pairs[self.batch_size * index: self.batch_size * (index + 1)]\n",
    "        neg_pairs = []\n",
    "\n",
    "        for i in range(len(pos_pairs)):\n",
    "            while True:\n",
    "                # sample question, its student and time\n",
    "                que, stu, zero = random.choice(self.ques_stus_times)\n",
    "                # calculate shift between question's and current time\n",
    "                shift = np.random.exponential(BatchGenerator.exp_mean)\n",
    "                current_time = zero + pd.Timedelta(int(shift * 24 * 60), 'm')\n",
    "                # find number of professionals with registration date before current time\n",
    "                i = np.searchsorted(self.pros_times, current_time)\n",
    "                if i != 0:\n",
    "                    break\n",
    "\n",
    "            while True:\n",
    "                # sample professional for negative pair\n",
    "                pro = random.choice(self.pros[:i])\n",
    "                # check if he doesn't form a positive pair\n",
    "                if (que, stu, pro) not in self.nonneg_pairs:\n",
    "                    neg_pairs.append((que, stu, pro, current_time))\n",
    "                    break\n",
    "\n",
    "        # convert lists of pairs to NumPy arrays of features\n",
    "        x_pos_que, x_pos_pro = self.__convert(pos_pairs)\n",
    "        x_neg_que, x_neg_pro = self.__convert(neg_pairs)\n",
    "\n",
    "        # return the data in its final form\n",
    "        return [np.vstack([x_pos_que, x_neg_que]), np.vstack([x_pos_pro, x_neg_pro])], \\\n",
    "               np.vstack([np.ones((len(x_pos_que), 1)), np.zeros((len(x_neg_que), 1))])\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # shuffle positive pairs\n",
    "        self.pos_pairs = random.sample(self.pos_pairs, len(self.pos_pairs))\n",
    "# mappings from professional's id to his registration date. Used in batch generator\n",
    "pro_to_date = {row['professionals_id']: row['professionals_date_joined'] for i, row in professionals.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = BatchGenerator(que_data, stu_data, pro_data, 64, pos_pairs, pos_pairs, pro_to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
